# -*- coding: utf-8 -*-
"""techinical_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QklTWulIQGMRtfuH_gp_SPjF0IN8QiRS

# üöÄ Case T√©cnico | Cientista de Dados - Galaxies

## ü™ê Contexto
- Base de consumidores com dados **demogr√°ficos, surveys e hist√≥rico de compras**.  
- Objetivo: **segmenta√ß√£o via clusteriza√ß√£o** para gerar **insights de neg√≥cio**.

## üìö Bibliotecas utilizadas
As seguintes bibliotecas foram utilizadas ao longo do projeto:

- **pandas (pd)**: manipula√ß√£o de dados em tabelas (DataFrames).  
- **numpy (np)**: opera√ß√µes matem√°ticas e vetoriais de alto desempenho.  
- **StandardScaler**: padroniza√ß√£o dos dados, ajustando para m√©dia 0 e desvio padr√£o 1.  
- **PCA**: redu√ß√£o de dimensionalidade, mantendo a maior parte da vari√¢ncia dos dados.  
- **KMeans**: algoritmo de clusteriza√ß√£o utilizado para segmentar os clientes.  
- **silhouette_score** e **cotovelo**: m√©tricas para avaliar a qualidade dos clusters formados.  
- **matplotlib.pyplot (plt)**: cria√ß√£o de gr√°ficos.  
- **seaborn (sns)**: visualiza√ß√µes estat√≠sticas mais claras e atrativas, baseada no matplotlib.  
- **datetime**: manipula√ß√£o de datas, como o c√°lculo de idade a partir da data de nascimento.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

"""## üîπ Carregamento e Explora√ß√£o Inicial da Base

A base de dados foi carregada em um DataFrame com `pandas`.  
Na sequ√™ncia, foram aplicados comandos de explora√ß√£o inicial:

- `head()` ‚Üí visualiza as primeiras linhas da base para conferir a estrutura.  
- `info()` ‚Üí exibe os tipos de dados de cada coluna e a presen√ßa de valores nulos.  
- `describe()` ‚Üí gera estat√≠sticas descritivas b√°sicas (m√©dia, desvio padr√£o, quartis, m√≠nimos e m√°ximos) para as vari√°veis num√©ricas.  

Esses passos iniciais s√£o fundamentais para validar a integridade e compreender a natureza dos dados antes do pr√©-processamento.

"""

# Ler a base de dados
df = pd.read_csv('dados_sinteticos.csv')

df.head()

df.info() #validar data types e informa√ßoes de dados nullos

df.describe()

"""## üîπ Limpeza de Dados Categ√≥ricos

Foi realizada uma limpeza simples diretamente no DataFrame para padronizar as vari√°veis categ√≥ricas:

- Convers√£o para **min√∫sculas** (`lower`)  
- Remo√ß√£o de **espa√ßos extras** nas pontas (`strip`)  

Em seguida, foi calculada a **cardinalidade** (n√∫mero de valores distintos) de cada coluna para identificar vari√°veis de alta diversidade e avaliar quais poderiam exigir tratamento espec√≠fico (ex.: agrupamento ou encoding alternativo).

"""

# Limpeza simples direto no dataframe
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

for col in categorical_cols:
    df[col] = (df[col]
                 .astype(str)
                 .str.lower()                 # min√∫sculas
                 .str.strip()                 # remove espa√ßos extras nas pontas
              )

# Agora sim calcular cardinalidade
distinct_counts = df.nunique().reset_index()
distinct_counts.columns = ["coluna", "num_valores_distintos"]
print(distinct_counts)

"""## üîπ Cria√ß√£o da Vari√°vel de Idade

A coluna `data_nascimento` foi convertida para o formato datetime e utilizada para calcular a idade de cada cliente.  
A opera√ß√£o consistiu em subtrair o ano de nascimento do ano atual, resultando em uma nova coluna `idade`.

Ap√≥s a cria√ß√£o da vari√°vel, foram exibidas estat√≠sticas descritivas (`describe()`) e informa√ß√µes de tipo de dado (`info()`) para validar o c√°lculo e garantir que n√£o houve inconsist√™ncias.

Esse passo transforma uma informa√ß√£o bruta (data) em uma vari√°vel num√©rica cont√≠nua, mais adequada para an√°lises estat√≠sticas e clusteriza√ß√£o.

"""

# Converter data de nascimento para datetime
hoje = datetime.now()

df['idade'] = hoje.year - pd.to_datetime(df['data_nascimento'], format='%d/%m/%Y').dt.year

# Exibir estat√≠sticas da idade
df['idade'].describe()

# Exibir estat√≠sticas da idade
df['idade'].info()

"""## üîπ Cria√ß√£o de Faixas Et√°rias

A vari√°vel `idade` foi transformada em **faixas et√°rias** por meio de `qcut`, que divide a distribui√ß√£o em quatro quantis.  
Cada faixa representa 25% da amostra, permitindo comparar grupos et√°rios de forma equilibrada.  

Em seguida, os intervalos gerados (objetos do tipo *Interval*) foram convertidos para **string**, facilitando sua manipula√ß√£o e posterior transforma√ß√£o em vari√°veis dummy.  

Essa segmenta√ß√£o torna a an√°lise mais interpret√°vel, permitindo identificar padr√µes de comportamento por grupo et√°rio.

"""

# Criar faixas et√°rias em 4 quantis
df['faixa_etaria'] = pd.qcut(df['idade'], q=4)

# Converter de Interval para string (ex.: "(17.999, 31.0]")
df['faixa_etaria'] = df['faixa_etaria'].astype(str)

# Conferir resultado
print(df['faixa_etaria'].value_counts())

"""## üîπ Cria√ß√£o da Vari√°vel Ordinal de Frequ√™ncia de Compra

A coluna `frequencia_compra` foi convertida em uma vari√°vel num√©rica ordinal (`frequencia_compra_ord`) por meio de um mapeamento personalizado:

- semanal = 0  
- quinzenal = 1  
- mensal = 2  
- bimestral = 3  
- trimestral = 4  

Essa transforma√ß√£o garante que a vari√°vel preserve a **ordem natural da frequ√™ncia** (do mais recorrente para o menos recorrente), permitindo que algoritmos como PCA e KMeans interpretem a informa√ß√£o de forma adequada.

"""

# Mapeamento ordinal personalizado
freq_map = {'semanal':0, 'quinzenal':1, 'mensal':2, 'bimestral':3, 'trimestral':4}
df['frequencia_compra_ord'] = df['frequencia_compra'].map(freq_map)

"""## üîπ Convers√£o Final em Vari√°veis Num√©ricas

Todas as colunas categ√≥ricas restantes foram convertidas em vari√°veis dummy (0/1) com `get_dummies`, garantindo que a base final ficasse totalmente num√©rica.  
Al√©m disso, valores nulos foram tratados com `fillna(0)`, assegurando que n√£o houvesse registros inv√°lidos que pudessem comprometer o PCA ou os algoritmos de clusteriza√ß√£o.

"""

# Converter todas as colunas categ√≥ricas restantes em vari√°veis dummy (0/1)
df_ready = pd.get_dummies(df, drop_first=True)

# Verificar e remover valores nulos (caso existam)
df_ready = df_ready.fillna(0)

"""## üîπ Padroniza√ß√£o dos Dados

Os dados foram padronizados com `StandardScaler`, ajustando todas as vari√°veis para m√©dia 0 e desvio padr√£o 1.  
Esse passo √© essencial para que nenhuma vari√°vel tenha peso desproporcional no c√°lculo de dist√¢ncias, garantindo que o PCA e o KMeans considerem todas as features de forma equilibrada.

"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_ready)

"""## üîπ Distribui√ß√µes"""

plt.figure(figsize=(6,4))
plt.hist(df['frequencia_compra'], bins=15, color='skyblue', edgecolor='black')
plt.title('Distribui√ß√£o de Frequ√™ncia de compra')
plt.xlabel('frequencia_compra')
plt.ylabel('Contagem')
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
plt.hist(df['faixa_etaria'], bins=15, color='skyblue', edgecolor='black')
plt.title('Distribui√ß√£o de Idade dos Clientes')
plt.xlabel('Idade (anos)')
plt.ylabel('Contagem')
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(x='genero', data=df, palette='pastel')
plt.title('Contagem por G√™nero')
plt.xlabel('G√™nero')
plt.ylabel('Contagem')
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(x='regiao', data=df, palette='pastel')
plt.title('Contagem por Regi√£o')
plt.xlabel('Regi√£o')
plt.ylabel('Contagem')
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.boxplot(x=df['ticket_medio'], color='lightgreen')
plt.title('Boxplot do Ticket M√©dio')
plt.xlabel('Ticket M√©dio')
plt.tight_layout()
plt.show()

"""### Clusteriza√ß√£o

## üîπ Tratamento de Dados Categ√≥ricos

Uma das principais vari√°veis de alta cardinalidade na base √© `influenciador`.  
Ao inv√©s de aplicar **One Hot Encoding** (que criaria milhares de colunas e dificultaria a clusteriza√ß√£o), adotamos **Frequency Encoding**.
"""

df['influenciador'] = df['influenciador'].map(df['influenciador'].value_counts(normalize=True))
df = df.drop('influenciador', axis=1)

df_original = df.copy()

display(df_original.head())

"""## üîπ Prepara√ß√£o da Base

As vari√°veis categ√≥ricas foram convertidas em dummies (0/1) e as num√©ricas mantidas no formato original.  
Esse passo garante que toda a base esteja em formato num√©rico, requisito para PCA e KMeans, al√©m de evitar distor√ß√µes nas medidas de dist√¢ncia.  
Em seguida, foram verificadas estat√≠sticas b√°sicas (m√©dia e desvio padr√£o) e o formato final da base para confirmar a prepara√ß√£o correta.

"""

categorical_cols = ['canal_preferido','categoria_favorita','regiao','pagamento','genero','faixa_etaria']
numeric_cols = ['ticket_medio','qtd_itens','idade','frequencia_compra_ord']

df_ready = pd.get_dummies(df[categorical_cols + numeric_cols], columns=categorical_cols, drop_first=False)


# Verificar estat√≠sticas ap√≥s padroniza√ß√£o (deve ter m√©dia ~0 e desvio ~1)
df_ready[numeric_cols].describe().loc[['mean','std']]

print(df_ready.shape)
df_ready.head()

"""## üîπ Padroniza√ß√£o e Redu√ß√£o de Dimensionalidade

Os dados foram padronizados com `StandardScaler` para garantir que todas as vari√°veis tenham a mesma escala.  
Em seguida, aplicou-se PCA para reduzir a dimensionalidade, mantendo no m√≠nimo **85% da vari√¢ncia explicada**.  
Isso elimina ru√≠dos, reduz a complexidade do espa√ßo e preserva as informa√ß√µes mais relevantes para a clusteriza√ß√£o.

"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_ready)

pca = PCA(n_components=0.85, svd_solver='full', random_state=42)
X_pca = pca.fit_transform(X_scaled)

print("Componentes selecionados:", pca.n_components_)
print("Vari√¢ncia explicada acumulada:", pca.explained_variance_ratio_.sum().round(2))

"""## üîπ Defini√ß√£o do N√∫mero de Clusters

Para escolher o n√∫mero ideal de clusters, foram aplicadas duas abordagens:

- **Silhouette Score**: mede a separa√ß√£o e coes√£o dos grupos (quanto maior, melhor).  
- **M√©todo do Cotovelo (In√©rcia)**: identifica o ponto onde o ganho marginal de vari√¢ncia explicada diminui.  

"""

sil_scores = []
inertias = []
K_range = range(2, 9)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(X_pca)

    sil = silhouette_score(X_pca, labels)

    sil_scores.append(sil)
    inertias.append(km.inertia_)

# --- Plot Silhouette ---
plt.figure(figsize=(6,4))
plt.plot(K_range, sil_scores, marker='o')
plt.title("Silhouette Score por K")
plt.xlabel("N√∫mero de Clusters (K)")
plt.ylabel("Silhouette")
plt.show()

# --- Plot m√©todo do cotovelo (In√©rcia) ---
plt.figure(figsize=(6,4))
plt.plot(K_range, inertias, marker='o')
plt.title("M√©todo do Cotovelo (In√©rcia)")
plt.xlabel("N√∫mero de Clusters (K)")
plt.ylabel("In√©rcia (dist√¢ncia intra-cluster)")
plt.show()

"""## üîπ Clusteriza√ß√£o Final - Kmeans


O KMeans define centr√≥ides, mede dist√¢ncias e agrupa. Isso facilita explicar o racional das rela√ß√µes entre as variaveis *

**DBSCAN:** √≥timo para ru√≠dos, mas exige calibra√ß√£o de par√¢metros (epsilon e min_samples) que poderia complicar o exerc√≠cio.

**Agglomerative/Hier√°rquico:** √∫til para dendrogramas, mas menos escal√°vel e mais dif√≠cil de explicar.

**GMM:** mais flex√≠vel que KMeans, mas adiciona complexidade estat√≠stica (probabilidades e elipses) sem necessidade para este case.


*A baixa silhueta geral diz que os segmentos de clientes n√£o est√£o fortemente separados ‚Äì houve muita sobreposi√ß√£o entre clusters at√© acertar as variaveis. Isso corrobora a impress√£o inicial de que n√£o existem grupos √≥bvios extremamente diferenciados neste dataset; em vez disso, a segmenta√ß√£o identificada √© sutil, baseada em combina√ß√µes de pequenas diferen√ßas nas muitas vari√°veis.*
"""

# Treinar KMeans com K=4 (config do resultado que voc√™ quer reproduzir)
k = 4
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_pca)

print("Tamanho dos clusters:\n", sizes)

# Plot 2 PCs
plt.figure(figsize=(6,4))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=labels, palette='viridis', s=20)
plt.title(f'KMeans (K={k})')
plt.xlabel('PC1'); plt.ylabel('PC2')
plt.legend(title="Cluster", bbox_to_anchor=(1.02,1), loc="upper left")
plt.show()

"""## üîπ Perfil Detalhado dos Clusters

Ap√≥s a clusteriza√ß√£o, foi constru√≠do o perfil de cada grupo de clientes a partir da pr√≥pria base usada no modelo.  
O resumo inclui:

- **Estat√≠sticas num√©ricas**: idade m√©dia, ticket m√©dio e quantidade m√©dia de itens por compra.  
- **Caracter√≠sticas categ√≥ricas dominantes**: canal de compra, categoria favorita, regi√£o, forma de pagamento, g√™nero e faixa et√°ria mais frequentes (com percentual de representatividade).  

Esse diagn√≥stico permite interpretar cada cluster como um segmento de clientes com comportamentos distintos, servindo de base para recomenda√ß√µes de neg√≥cio.

"""

# ===== PERFIL DETALHADO usando df_used (mesma base e mesmas labels) =====
print("\nPERFIL DETALHADO DE CADA CLUSTER")
print("=================================")

cat_cols_profile = [c for c in ['canal_preferido','categoria_favorita','regiao','pagamento','genero','faixa_etaria'] if c in df_used.columns]
num_cols_profile = [c for c in ['idade','ticket_medio','qtd_itens'] if c in df_used.columns]

for c_id in sorted(df_used['cluster'].unique()):
    cluster_data = df_used[df_used['cluster'] == c_id]
    print(f"\n{'='*50}")
    print(f"CLUSTER {c_id} (Tamanho: {len(cluster_data)} clientes)")
    print(f"{'='*50}")

    # estat√≠sticas num√©ricas
    for col in num_cols_profile:
        print(f"‚Ä¢ {col}: {cluster_data[col].mean():.2f}")

    # modas categ√≥ricas (valor + %)
    print("\nCARACTER√çSTICAS CATEG√ìRICAS MAIS FREQUENTES:")
    for var in cat_cols_profile:
        moda = cluster_data[var].mode()
        if not moda.empty:
            val = moda.iloc[0]
            pct = (cluster_data[var].eq(val).mean()*100)
            print(f"‚Ä¢ {var}: {val} ({pct:.1f}%)")